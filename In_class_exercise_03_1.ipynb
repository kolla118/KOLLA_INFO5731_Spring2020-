{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kolla118/KOLLA_INFO5731_Spring2020-/blob/main/In_class_exercise_03_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCtaMmzja-LB"
      },
      "source": [
        "## The third In-class-exercise (2/22/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ArVe53La-LC"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbSMagXa-LC"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcgSLg3Ca-LC"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "A good example of a text classification task would be sentiment analysis of a given text document or piece of writing. The sentiment analysis can be about the author's sentiment about a topic and the relative strength of his sentiment.\n",
        "The following features types are considered in building the machine learning model:\n",
        "1. Bag of words (preprocessed to remove spaces,special characters and single character words) based on a bag of words model. The more frequent the word, the better it impact it has on the sentiment. Rare and in-frequent words don't reflect the author's sentiment as they are too specific or random.\n",
        "2. Word count (the number of words in the document) - is a reliable indicator of how strongly the author feels about the text he/she writes\n",
        "3. Character count (the nuumber of characters in the document) - is good differentiator based on length analysis\n",
        "4. Sentence count (the number of sentences in the document) - is good differentiator based on length analysis \n",
        "5. Average Sentence Length (the average sentence length) - is good differentiator based on length analysis \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73DbMWR9a-LD"
      },
      "source": [
        "Question 2 (20 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWKYSN94a-LD",
        "outputId": "026cfaca-c3ef-46ff-9672-327815de243e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "                                             features labels\n",
            "0                                                          0\n",
            "1                                                          1\n",
            "2   ordered directly shipped gift relative arrived...      2\n",
            "3                                                          3\n",
            "4                                                          4\n",
            "5                                                          5\n",
            "6   best cooky ever friend gave box christmas son ...      6\n",
            "7                                                          7\n",
            "8                                                          8\n",
            "9                                                          9\n",
            "10  ordered winter received melted product seems c...     10\n",
            "11                                                        11\n",
            "12                                                        12\n",
            "13                                                        13\n",
            "14  hate stop eating know fattening came perfect c...     14\n",
            "15                                                        15\n",
            "16                                                        16\n",
            "17                                                        17\n",
            "18  wa nervous receiving order read review purchas...     18\n",
            "19                                                        19\n",
            "20                                                        20\n",
            "21                                                        21\n",
            "22  order cooky summertime warm weather live fl co...     22\n",
            "23                                                        23\n",
            "24                                                        24\n",
            "25                                                        25\n",
            "26  received box cooky melted messy even lift cook...     26\n",
            "27                                                        27\n",
            "28                                                        28\n",
            "29                                                        29\n",
            "30  saved cooky grandchild new year day horrible t...     30\n",
            "31                                                        31\n",
            "32                                                        32\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    word_count labels\n",
            "0            0      0\n",
            "1            0      1\n",
            "2           29      2\n",
            "3            0      3\n",
            "4            0      4\n",
            "5            0      5\n",
            "6           58      6\n",
            "7            0      7\n",
            "8            0      8\n",
            "9            0      9\n",
            "10          20     10\n",
            "11           0     11\n",
            "12           0     12\n",
            "13           0     13\n",
            "14          45     14\n",
            "15           0     15\n",
            "16           0     16\n",
            "17           0     17\n",
            "18          55     18\n",
            "19           0     19\n",
            "20           0     20\n",
            "21           0     21\n",
            "22         115     22\n",
            "23           0     23\n",
            "24           0     24\n",
            "25           0     25\n",
            "26          16     26\n",
            "27           0     27\n",
            "28           0     28\n",
            "29           0     29\n",
            "30          32     30\n",
            "31           0     31\n",
            "32           0     32\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    char_count labels\n",
            "0            0      0\n",
            "1            0      1\n",
            "2          191      2\n",
            "3            0      3\n",
            "4            0      4\n",
            "5            0      5\n",
            "6          286      6\n",
            "7            0      7\n",
            "8            0      8\n",
            "9            0      9\n",
            "10         109     10\n",
            "11           0     11\n",
            "12           0     12\n",
            "13           0     13\n",
            "14         262     14\n",
            "15           0     15\n",
            "16           0     16\n",
            "17           0     17\n",
            "18         273     18\n",
            "19           0     19\n",
            "20           0     20\n",
            "21           0     21\n",
            "22         650     22\n",
            "23           0     23\n",
            "24           0     24\n",
            "25           0     25\n",
            "26          85     26\n",
            "27           0     27\n",
            "28           0     28\n",
            "29           0     29\n",
            "30         170     30\n",
            "31           0     31\n",
            "32           0     32\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    sentence_count labels\n",
            "0                1      0\n",
            "1                1      1\n",
            "2                6      2\n",
            "3                1      3\n",
            "4                1      4\n",
            "5                1      5\n",
            "6               16      6\n",
            "7                1      7\n",
            "8                1      8\n",
            "9                1      9\n",
            "10               3     10\n",
            "11               1     11\n",
            "12               1     12\n",
            "13               1     13\n",
            "14               9     14\n",
            "15               1     15\n",
            "16               1     16\n",
            "17               1     17\n",
            "18              10     18\n",
            "19               1     19\n",
            "20               1     20\n",
            "21               1     21\n",
            "22              26     22\n",
            "23               1     23\n",
            "24               1     24\n",
            "25               1     25\n",
            "26               4     26\n",
            "27               1     27\n",
            "28               1     28\n",
            "29               1     29\n",
            "30               9     30\n",
            "31               1     31\n",
            "32               1     32\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "    average_sentence_length labels\n",
            "0                  0.000000      0\n",
            "1                  0.000000      1\n",
            "2                  4.833333      2\n",
            "3                  0.000000      3\n",
            "4                  0.000000      4\n",
            "5                  0.000000      5\n",
            "6                  3.625000      6\n",
            "7                  0.000000      7\n",
            "8                  0.000000      8\n",
            "9                  0.000000      9\n",
            "10                 6.666667     10\n",
            "11                 0.000000     11\n",
            "12                 0.000000     12\n",
            "13                 0.000000     13\n",
            "14                 5.000000     14\n",
            "15                 0.000000     15\n",
            "16                 0.000000     16\n",
            "17                 0.000000     17\n",
            "18                 5.500000     18\n",
            "19                 0.000000     19\n",
            "20                 0.000000     20\n",
            "21                 0.000000     21\n",
            "22                 4.423077     22\n",
            "23                 0.000000     23\n",
            "24                 0.000000     24\n",
            "25                 0.000000     25\n",
            "26                 4.000000     26\n",
            "27                 0.000000     27\n",
            "28                 0.000000     28\n",
            "29                 0.000000     29\n",
            "30                 3.555556     30\n",
            "31                 0.000000     31\n",
            "32                 0.000000     32\n"
          ]
        }
      ],
      "source": [
        "from os import defpath\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "HEADERS = ({'User-Agent':\n",
        "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
        "            AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
        "            Chrome/90.0.4430.212 Safari/537.36',\n",
        "            'Accept-Language': 'en-US, en;q=0.5'})\n",
        "labels = []\n",
        "word_cnt_list = []\n",
        "char_cnt_list = []\n",
        "sentence_cnt_list = []\n",
        "avg_sen_len_list = []\n",
        "\n",
        "\n",
        "\n",
        "def word_count(document):\n",
        "  return len(document)\n",
        "\n",
        "def char_count(document):\n",
        "  val = 0\n",
        "  for word in document:\n",
        "    val += len(word)\n",
        "  return val\n",
        "\n",
        "def sentence_count(str):\n",
        "  return len(str.split(\".\"))\n",
        "\n",
        "\n",
        "def get_customer_reviews(soup):\n",
        "  str = \"\"\n",
        "\n",
        "  scraped_review_list = soup.find_all(\"div\",{\"data-hook\":\"review-collapsed\"})\n",
        "  #print(scraped_review_list)\n",
        "  \n",
        "  for review in scraped_review_list:\n",
        "    str = str + review.get_text()\n",
        "  #print(str)\n",
        "  return str.split(\"\\n\")\n",
        "\n",
        "\n",
        "def get_documents(url):\n",
        "  \n",
        "  raw_html =  requests.get(url, headers=HEADERS).text\n",
        "  #print(raw_html)\n",
        "  soup = BeautifulSoup(raw_html, 'lxml')\n",
        "  return get_customer_reviews(soup)\n",
        "\n",
        "\n",
        "def get_lemmatized_docs(X):\n",
        "  stemmer = WordNetLemmatizer()\n",
        "  eng_stop_word_list = stopwords.words(\"english\")\n",
        "  bag_of_words = []\n",
        "  for sen in range(0, len(X)):\n",
        "      # Remove all the special characters\n",
        "      document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "      document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "      document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "      document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "      document = re.sub(r'^b\\s+', '', document)\n",
        "      document = document.lower()\n",
        "      \n",
        "      # Splitting the cleaned document into a list of words, which will be subsequently added to a bag after more processing\n",
        "      document = document.split()\n",
        "      document = [stemmer.lemmatize(word) for word in document]\n",
        "      document = [word for word in document if word not in eng_stop_word_list]\n",
        "      \n",
        "      #Bag of words\n",
        "      bag_of_words.append(document)\n",
        "\n",
        "      # Labels\n",
        "      labels.append(str(sen))\n",
        "\n",
        "      # Word count\n",
        "      word_cnt_list.append(word_count(document))\n",
        "\n",
        "      # Character count\n",
        "      char_cnt_list.append(char_count(document))\n",
        "\n",
        "      # Sentence count\n",
        "      sentence_cnt_list.append(sentence_count(str(X[sen])))\n",
        "\n",
        "      # Average Sentence Length\n",
        "      if(sentence_cnt_list[-1] == 0):\n",
        "        avg_sen_len_list.append(0)\n",
        "      else:\n",
        "        avg_sen_len_list.append(word_cnt_list[-1]/sentence_cnt_list[-1])\n",
        "      \n",
        "      # bag_of_words contains all the useful features or terms from the original document\n",
        "  return bag_of_words\n",
        "\n",
        "\n",
        "documents = get_documents(\"https://www.amazon.com/Kirkland-Signature-European-Cookies-Chocolate/dp/B003ZIR8YU?pd_rd_w=fQvtk&pf_rd_p=cf1a5a84-8a64-4d2b-b13a-eedb7e66da7d&pf_rd_r=VY2H4Y5276PSSE8782H8&pd_rd_r=6619c159-1009-439a-aadf-3a0aba1eb5a2&pd_rd_wg=vgrNq&pd_rd_i=B003ZIR8YU&ref_=pd_bap_d_rp_1_i&th=1\")\n",
        "cleaned_documents = get_lemmatized_docs(documents)\n",
        "#print(cleaned_documents)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "df[\"features\"] = cleaned_documents\n",
        "df[\"features\"] = df[\"features\"].apply(lambda x: \" \".join(x))\n",
        "df[\"labels\"] = labels\n",
        "df[\"word_count\"] = word_cnt_list\n",
        "df[\"char_count\"] = char_cnt_list\n",
        "df[\"sentence_count\"] = sentence_cnt_list\n",
        "df[\"average_sentence_length\"] = avg_sen_len_list;\n",
        "#print(df)\n",
        "\n",
        "\n",
        "df1 = pd.DataFrame()\n",
        "df1['features'] = df[\"features\"]\n",
        "df1[\"labels\"] = labels\n",
        "print(df1)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "df2 = pd.DataFrame()\n",
        "df2[\"word_count\"] =df[\"word_count\"]\n",
        "df2[\"labels\"] = labels\n",
        "print(df2)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "df3 = pd.DataFrame()\n",
        "df3[\"char_count\"] =df[\"char_count\"]\n",
        "df3[\"labels\"] = labels\n",
        "print(df3)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "df4 = pd.DataFrame()\n",
        "df4[\"sentence_count\"] =df[\"sentence_count\"]\n",
        "df4[\"labels\"] = labels\n",
        "print(df4)\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"\")\n",
        "\n",
        "df5 = pd.DataFrame()\n",
        "df5[\"average_sentence_length\"] =df[\"average_sentence_length\"]\n",
        "df5[\"labels\"] = labels\n",
        "print(df5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpJk4Jeqa-LD"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kpbRGzGa-LE",
        "outputId": "80a827f5-ecde-4980-9e11-e676b3df1e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         term  cumulative_TF-IDF\n",
            "44      cooky           1.227403\n",
            "136    melted           0.961998\n",
            "148     order           0.756456\n",
            "215        wa           0.688905\n",
            "92       good           0.587032\n",
            "..        ...                ...\n",
            "187  shipping           0.096938\n",
            "140    minute           0.096938\n",
            "138      mess           0.096938\n",
            "192   steered           0.096938\n",
            "0          15           0.096938\n",
            "\n",
            "[225 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Utilizing the TF-IDF filter model mention \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfconverter = TfidfVectorizer(analyzer='word', stop_words = 'english')\n",
        "X = tfidfconverter.fit_transform(df[\"features\"])\n",
        "\n",
        "terms = tfidfconverter.get_feature_names()\n",
        "\n",
        "# sum tfidf frequency of each term through documents\n",
        "sums = X.sum(axis=0)\n",
        "\n",
        "data = []\n",
        "for col, term in enumerate(terms):\n",
        "    data.append( (term, sums[0,col] ))\n",
        "\n",
        "sorted_features = pd.DataFrame(data, columns=['term','cumulative_TF-IDF'])\n",
        "print(sorted_features.sort_values('cumulative_TF-IDF', ascending=False))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-03-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}